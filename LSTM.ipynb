{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the dataset\n",
    "df = pd.read_csv(\"./df_file.csv\")\n",
    "\n",
    "X = df['Text'].tolist()\n",
    "y = df['Label'].tolist()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text (fit on training data only)\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to have consistent length\n",
    "max_len = 100  # You can adjust this based on your dataset\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "embedding_dim = 50 \n",
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.1917 - accuracy: 0.9482 - val_loss: 0.7772 - val_accuracy: 0.8146\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.1168 - accuracy: 0.9744 - val_loss: 0.4575 - val_accuracy: 0.8539\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.1134 - accuracy: 0.9813 - val_loss: 0.8270 - val_accuracy: 0.7303\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.2126 - accuracy: 0.9719 - val_loss: 0.5037 - val_accuracy: 0.8371\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.0663 - accuracy: 0.9906 - val_loss: 0.5113 - val_accuracy: 0.8596\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.0266 - accuracy: 0.9969 - val_loss: 0.5069 - val_accuracy: 0.8708\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.0247 - accuracy: 0.9944 - val_loss: 0.8347 - val_accuracy: 0.8315\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.6732 - accuracy: 0.8920 - val_loss: 0.6143 - val_accuracy: 0.8258\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.0466 - accuracy: 0.9925 - val_loss: 0.4449 - val_accuracy: 0.8708\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.0885 - accuracy: 0.9806 - val_loss: 0.5465 - val_accuracy: 0.8371\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.0235 - accuracy: 0.9963 - val_loss: 0.5046 - val_accuracy: 0.8483\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.0205 - accuracy: 0.9969 - val_loss: 0.4905 - val_accuracy: 0.8539\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.0122 - accuracy: 0.9981 - val_loss: 0.5402 - val_accuracy: 0.8539\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.0139 - accuracy: 0.9981 - val_loss: 0.4835 - val_accuracy: 0.8596\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.4604 - val_accuracy: 0.8708\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.2301 - accuracy: 0.9132 - val_loss: 0.6287 - val_accuracy: 0.7921\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.0925 - accuracy: 0.9757 - val_loss: 0.5310 - val_accuracy: 0.8371\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.0296 - accuracy: 0.9950 - val_loss: 0.8163 - val_accuracy: 0.7865\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.0108 - accuracy: 0.9988 - val_loss: 0.6112 - val_accuracy: 0.8652\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6717 - val_accuracy: 0.8371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268439bbca0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 36ms/step - loss: 0.6541 - accuracy: 0.8449\n",
      "Test accuracy: 0.8449438214302063\n",
      "14/14 [==============================] - 0s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.89      0.75        84\n",
      "           1       0.92      0.91      0.92       102\n",
      "           2       0.97      0.71      0.82        80\n",
      "           3       0.84      0.84      0.84        77\n",
      "           4       0.92      0.84      0.88       102\n",
      "\n",
      "    accuracy                           0.84       445\n",
      "   macro avg       0.86      0.84      0.84       445\n",
      "weighted avg       0.87      0.84      0.85       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
